{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0a98ecc",
   "metadata": {},
   "source": [
    "### Twitter scraper \n",
    "\n",
    "This notebook scrapes tweets by using the snscrape module. The parameters below work as follows: \n",
    "- **maxTweets**: Number of daily tweets you want to download (the goal was to scrape all tweets, but I limited the amount to 1M)\n",
    "- **datelist**: List of dates when you want to query\n",
    "- **query_text**: Text to search for (Ukraine, in this case)\n",
    "\n",
    "The goal of this notebook was to scrape all tweets for 4 days in december (from 01/12/21 to 04/12/21) as a \"control group\" and 4 days in the period of the conflict (from 23/02/22 to 26/02/22) for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f809f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef86fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxTweets = 1000000\n",
    "datelist = ['Dec 01 2021', 'Dec 02 2021', 'Dec 03 2021', 'Dec 04 2021',\n",
    "             'Feb 23 2022', 'Feb 24 2022', 'Feb 25 2022', 'Feb 26 2022']\n",
    "\n",
    "query_text = 'Ukraine'\n",
    "for date in datelist:\n",
    "\n",
    "    datetime_object = datetime.strptime(date, '%b %d %Y') \n",
    "\n",
    "    sd = str(int(datetime_object.timestamp())) \n",
    "    ed = str(int((datetime_object + timedelta(days=1)).timestamp())) \n",
    "\n",
    "    df_coords = pd.DataFrame((sntwitter.TwitterSearchScraper(\n",
    "        f'{query_text} since:{sd} until:{ed}').get_items()))[['date', 'content','user', 'coordinates', 'place']]\n",
    "\n",
    "    df_coords.to_csv(f'df_with_coors_Ukraine_{date}.csv', sep = '~')\n",
    "    print(len(df_coords), date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d3abd5",
   "metadata": {},
   "source": [
    "Notes on saving the files: \n",
    "- Since the scraping process is pretty slow and we do not want to restart in case of a memory error, I am saving a different dataframe for every day and re-writing on the same name (df_coords) so that we don't have to keep everything in memory\n",
    "- Saving with tilde (~) separator to avoid any tweet splitting up because the user added a '\\n' or a ';' in their tweet! I suggest to keep it like this and just read the csv by specifying the same separator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cdd185",
   "metadata": {},
   "source": [
    "More info at: \n",
    "\n",
    "https://github.com/igorbrigadir/twitter-advanced-search \\\n",
    "https://medium.com/swlh/how-to-scrape-tweets-by-location-in-python-using-snscrape-8c870fa6ec25"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
